# R code Parallelization

## Introduction to parallel execution in `r fontawesome::fa("r-project")`

Apart from optimizing the code and the algorithms, another way to get a high performing code is to take advantage of the parallel architectures of modern computers. The goal is then to **parallelize** a code in order to perform simultaneous operations on distinct parts of the same problem, using different computing cores. This does not reduce the total computation time needed, but the set of operations is executed faster, resulting in an overall user speed-up.

There is a significant number of algorithms that are so-called "*embarrassingly parallel*", i.e. whose computations can be broken down into several independent sub-computations. In statistics, it is often easy and straightforward to parallelize according to different observations or different dimensions. Typically, these are operations that can be written in the form of a loop whose operations are independent from one iteration to the next.

**The necessary operations to execute code in parallel are as follows:**

  1. Start $m$ "worker" processes (i.e. computing cores) and initialize them
  
  2. Send the necessary functions and data for each task to the workers
  
  3. Split the tasks into $m$ operations of similar size and send them to the workers
  
  4. Wait for all workers to finish their calculations
  
  5. Collect the results from the different workers
  
  6. Stop the worker processes


Depending on the platform, several communication protocols are available between cores. Under UNIX systems, the *Fork* protocol is the most used (it has the benefit of sharing memory), but it is not available under Windows where the *PSOCK* protocol is used instead (with duplicated memory). Finally, for distributed computing architecture where the cores are not necessarily on the same physical processor, the *MPI* protocol is generally used. The advantage of the `future` and `future.apply` package is that the same code can be executed whatever the hardware configuration.

In `r fontawesome::fa("r-project")` the `parallel` package allows to start and stop a cluster of several worker processes (step 1 and 6). In addition to the `parallel` package, we will use the `future` package which allows to manage the worker processes and the communication and articulation with the `future.apply` package, which in turn allows to manage the dialogue with the workers (sending, receiving and collecting the results -- steps 2, 3, 4 and 5).


**NB:** The free plan on *Posit Cloud* only includes 1 core.

## First `r fontawesome::fa("r-project")` parallel function 

> `r emoji::emoji("point_right")` ***Your turn!*** 
>
> Start by writing a simple function that computes the logarithm of $n$ numbers:
>
>  1. Determine how many cores are available on your computer with the function `future::availableCores()` (under Linux, prefer `parallel::detectCores(logical=FALSE)` to avoid overthreading).
>
>  2. Load the `future` package and using the function `future::plan(multisession(workers = XX))`, declare a "*plan*" of parallel computations on your computer (take care to **always leave at least one core available** to handle other processes).
>
>  3. Using one of the *\*apply* functions `future.apply::future_*apply()`, compute the log of $n$ numbers in parallel and concatenate the results into a vector.
>
>  4. Compare the execution time with that of a sequential function on the first 100 integers, using the command :  
`microbenchmark(log_par(1:100), log_seq(1:100), times=10)`


```{r echo = TRUE, message = FALSE, cache=TRUE}
library(microbenchmark)
library(future.apply)

log_seq <- function(x) {
  # try this yourself (spoiler alert: it is quite long...):
  # res <- numeric(length(x))
  # for(i in 1:length(x)){
  #   res[i] <- log(x[i])
  # }
  # return(res)
  return(log(x))
}

log_par <- function(x) {
  res <- future_sapply(1:length(x), FUN = function(i) {
    log(x[i])
  })
  return(res)
}

plan(multisession(workers = 3))
mb <- microbenchmark(log_par(1:100), log_seq(1:100), times = 50)
```


```{r, echo=FALSE}
library(ggplot2)
data2plot <- cbind.data.frame("Time" = mb$time/10^6, "Expression" = mb$expr)
rangeTime <- c(floor(log10(min(data2plot$Time))):ceiling(log10(max(data2plot$Time))))
brk <- NULL
for(i in rangeTime){
    brk <- c(brk, seq(from = 10^i, to = 10^(i+1), by=10^i))
}
ggplot(data2plot) + geom_violin(aes(x=Expression, y=Time, fill=Expression), alpha=0.8)  +
  scale_fill_manual(guide="none", values=viridis::viridis(6)[1:4]) + 
  scale_y_log10(minor_breaks=brk) + 
  ylab("Execution timings (milli-sec)") +
  xlab("Computed expression") +
  annotation_logticks(sides="l") +
  theme_bw() +
  ggtitle("Spoiler alert")
```

The parallel version runs much slower... Indeed, since the individual tasks are too fast, `r fontawesome::fa("r-project")` will spend more time communicating with the cores than doing the actual computations. Hence:

**A loop iteration must be relatively long for parallel computing to provide a significant gain in computation time!**

By increasing $n$, we observe a reduction in the difference between the 2 implementations (the parallel computation time increases very slowly compared to the increase of the sequential function).

<!--**NB:** the iterators of the `itertools` package are very efficient to minimize the number of communications between cores, but can only be used when the code inside `future_*apply()` is vectorized (it is always possible to vectorize the code inside, for example with a function of type `apply`).-->




## Efficient parallelization

We will now look at another use case. Let's say we have a large array of data with 10 observations for 100,000 variables (e.g. genetic measurements), and we want to compute the median for each of these variables.

```{r}
x <- matrix(rnorm(1e6), nrow = 10)
dim(x)
```

For an experienced `r fontawesome::fa("r-project")` user, such an operation is easily implemented using `apply()`:

```{r}
colmedian_apply <- function(x){
  return(apply(X = x, MARGIN = 2, FUN = median))
}
system.time(colmedian_apply(x))
```

In reality, a (good) `for` loop is no slower -- provided it is nicely programmed:

```{r}
colmedian_for <- function(x){
  p <- ncol(x)
  ans <- numeric(p)
  for (i in 1:p) {
    ans[i] <- median(x[, i]) 
  }
  return(ans)
}
system.time(colmedian_for(x))
```



> `r emoji::emoji("point_right")` ***Your turn!*** 
> Try to further improve this computation time by parallelizing your code for each of the 100,000 variables. Is there a gain in computation time?

<!-- 2. Propose an alternative implementation using the `itertools::isplitIndices()` function which allows you to separate your data (the $n$ numbers) into as many groups (or batchs) as you have cores. Compare again the computation times.
-->

```{r, error=TRUE, cache=TRUE}
colmedian_par <- function(x){
  res <- future_sapply(1:ncol(x), FUN = function(i) {
          median(x[, i])
    })
  return(res)
}
plan(multisession(workers = 3))
system.time(colmedian_par(x))

mb <- microbenchmark(colmedian_apply(x), 
                     colmedian_for(x),
                     colmedian_par(x), 
                     times = 10)
mb
```

```{r, echo=FALSE}
library(ggplot2)
data2plot <- cbind.data.frame("Time" = mb$time/10^9, "Expression" = mb$expr)
rangeTime <- c(floor(log10(min(data2plot$Time))):ceiling(log10(max(data2plot$Time))))
brk <- NULL
for(i in rangeTime){
    brk <- c(brk, seq(from = 10^i, to = 10^(i+1), by=10^i))
}
ggplot(data2plot) + geom_violin(aes(x=Expression, y=Time, fill=Expression), alpha=0.8)  +
  scale_fill_manual(guide="none", values=viridis::viridis(6)[1:4]) + 
  #scale_y_log10(minor_breaks=brk) + 
  ylab("Execution timings (sec)") +
  xlab("Computed expression") +
  #annotation_logticks(sides="l") +
  theme_bw() +
  ylim(0, max(data2plot$Time)) +
  ggtitle("Parallelizing works! 3 cores used for parallle computations")
```

<!--
###  Iterators

The `itertools` package allows to easily split data or tasks (step 3) while minimizing communication with different workers. It is based on an implementation of iterators in `r fontawesome::fa("r-project")`. However, its use requires vectorizing the code inside `future_*apply()`. Experiment with the small code below: 

```{r, error = TRUE}
myiter <- itertools::isplitIndices(n = 30, chunks = 3)

# a first time
iterators::nextElem(myiter)
# a second time... Oh ?!
iterators::nextElem(myiter)
# again !
iterators::nextElem(myiter)
# again ?
iterators::nextElem(myiter)
```
-->

### Other "plans" for parallel computations

To run your code (exactly the same code, this is one of the advantages of the `future*` family packages), you need to set up a "plan" of computations:

- On a computer (or a single computer server) under Unix (`r fontawesome::fa("linux")` Linux, `r fontawesome::fa("apple")` MacOS), you can use `plan(multicore(workers = XX))` which is often more efficient. The `multisession` plan always works.

- On an HPC cluster (like CURTA in Bordeaux), we refer to the package [`future.batchtools`](https://cran.r-project.org/web/packages/future.batchtools/vignettes/future.batchtools.html)



## Parallelization in our common theme example

> `r emoji::emoji("point_right")` ***Your turn!*** 
>
>  1. From the function `mvnpdfoptim()` and/or `mvnpdfsmart()`, propose an implementation parallelizing the computations on the observations (columns of $x$)
>  
>  2. Compare the execution times for 10 000 observations


```{r, cache=TRUE}
plan(multisession(workers = 3))
n <- 10000
mb <- microbenchmark::microbenchmark(
  mypkgr::mvnpdfsmart(x=matrix(1.96, nrow = 2, ncol = n), Log=FALSE),
  mypkgr::mvnpdfsmart_par(x=matrix(1.96, nrow = 2, ncol = n), Log=FALSE),
  times=20)
mb
```

```{r, echo=FALSE}
library(ggplot2)
data2plot <- cbind.data.frame("Time" = mb$time/10^6, "Expression" = mb$expr)
rangeTime <- c(floor(log10(min(data2plot$Time))):ceiling(log10(max(data2plot$Time))))
brk <- NULL
for(i in rangeTime){
    brk <- c(brk, seq(from = 10^i, to = 10^(i+1), by=10^i))
}
levels(data2plot$Expression) <- gsub("mvtnorm::", "", sapply(strsplit(levels(data2plot$Expression), "(", fixed=T), "[", 1))
ggplot(data2plot) + geom_violin(aes(x=Expression, y=Time, fill=Expression), alpha=0.8)  +
  scale_fill_manual(guide="none", values=viridis::viridis(6)[1:4]) + 
  scale_y_log10(minor_breaks=brk) + 
  ylab("Execution timings (milli-sec)") +
  xlab("Computed expression") +
  annotation_logticks(sides="l") +
  theme_bw() +
  ggtitle("Spoiler alert 2")
```

You can download our proposed implementation for `mvnpdfsmart_par` [here](https://github.com/borishejblum/mypkgr/blob/ddd6a36c1c8b60c21a51bcb8760caa73459c57af/R/mvnpdfsmart_par.R).

In this example, the computation time of one iteration is really too fast for the parallel version to be efficient. We artificially add some time on each iteration to illustrate the gain of parallel computing for this example.

```{r, cache=TRUE}
plan(multisession(workers = 3))
n <- 10000
system.time(mypkgr::mvnpdfsmart_sleepy(x=matrix(1.96, nrow = 2, ncol = n), Log=FALSE))
system.time(mypkgr::mvnpdfsmart_sleepy_par(x=matrix(1.96, nrow = 2, ncol = n), Log=FALSE))
```

You can finally clone (or download or fork) our [GitHub repository](https://github.com/borishejblum/mypkgr) to get all the different implementations directly.

## Conclusion

Parallel computation saves time, but first you have to optimize your code. When parallelizing a code, the gain on the execution time depends above all on the ratio between the communication time and the effective computation time for each task.

## Annexe 6.1: Progress bar {-}

The [`pbapply` package](https://peter.solymos.org/pbapply/) allows to get a nice progress bar for parallelized `*apply` functions execution, and its parallelization capabilities are relatively straightforward to use (very straightforward on both `r fontawesome::fa("linux")` Linux, `r fontawesome::fa("apple")` MacOS, slightly more involved on `r fontawesome::fa("windows")` Windows). Try it out !

